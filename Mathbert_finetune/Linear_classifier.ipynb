{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "def load_embeddings(file_path):\n",
        "    data = torch.load(file_path, map_location=torch.device('cpu'))\n",
        "    embeddings = data['embeddings']  # Shape: (num_samples, embedding_dim)\n",
        "    labels = data['labels']          # Shape: (num_samples,)\n",
        "    return embeddings, labels\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/fine_tuned_train_embeddings.pt\"\n",
        "file_path2 = \"/content/drive/MyDrive/fine_tuned_val_embeddings.pt\"\n",
        "file_path3 = \"/content/drive/MyDrive/fine_tuned_test_embeddings.pt\"\n",
        "train_embeddings, train_labels = load_embeddings(file_path1)\n",
        "val_embeddings, val_labels = load_embeddings(file_path2)\n",
        "test_embeddings, test_labels = load_embeddings(file_path3)\n",
        "train_Y= torch.tensor(train_labels.values) if isinstance(train_labels, pd.Series) else torch.tensor(train_labels)\n",
        "val_Y = torch.tensor(val_labels.values) if isinstance(val_labels, pd.Series) else torch.tensor(val_labels)\n",
        "test_Y = torch.tensor(test_labels.values) if isinstance(test_labels, pd.Series) else torch.tensor(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftCHC-Wu3T9F",
        "outputId": "7608d195-ee29-44e1-e10c-d08a780c1e3f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-24d60ef19bbb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(file_path, map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "file_path2 = '/content/drive/MyDrive/finalDataSet.csv'\n",
        "df = pd.read_csv(file_path2)\n",
        "Y = df['MisconceptionId'].astype(int)\n",
        "label_encoder = LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "print(Y[:5])\n",
        "Y_tensor = torch.tensor(Y.values) if isinstance(Y, pd.Series) else torch.tensor(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy4JNs6u4aDe",
        "outputId": "af6bd5b5-edda-45b0-90e5-134c5fa36051"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1347  792  724  425  207]\n",
            "tensor([1347,  792,  724,  425,  207])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple linear classifier model\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Get the input size from embeddings and the number of classes from Y\n",
        "input_dim = test_embeddings.shape[1]  # Embedding dimension\n",
        "max_label_value = Y_tensor.max().item()\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearClassifier(input_dim=input_dim, output_dim=max_label_value + 1)\n"
      ],
      "metadata": {
        "id": "aFSIvWV-4OTw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "learning_rate = 1e-5\n",
        "batch_size = 32\n",
        "num_epochs = 250\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Shuffle training data at the beginning of each epoch\n",
        "    perm = torch.randperm(train_embeddings.shape[0])\n",
        "    train_embeddings_shuffled = train_embeddings[perm]\n",
        "    train_Y_shuffled = train_Y[perm]\n",
        "\n",
        "    # Process inputs in batches for training\n",
        "    for i in range(0, train_embeddings.shape[0], batch_size):\n",
        "        batch_embeddings = train_embeddings_shuffled[i:i + batch_size]\n",
        "        batch_labels = train_Y_shuffled[i:i + batch_size]\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_embeddings)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation every 10 epochs\n",
        "    if (epoch + 1) % 25 == 0:\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_embeddings)\n",
        "            _, val_preds = torch.max(val_outputs, dim=1)\n",
        "            val_accuracy = accuracy_score(val_Y.cpu(), val_preds.cpu())  # Calculate accuracy\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1zlkr0F4PwD",
        "outputId": "6b075b4e-1774-42bb-d096-615332c657c0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/250], Loss: 539.5733, Validation Accuracy: 0.2174\n",
            "Epoch [50/250], Loss: 343.5833, Validation Accuracy: 0.2426\n",
            "Epoch [75/250], Loss: 227.7819, Validation Accuracy: 0.2654\n",
            "Epoch [100/250], Loss: 160.5357, Validation Accuracy: 0.2654\n",
            "Epoch [125/250], Loss: 120.9631, Validation Accuracy: 0.2609\n",
            "Epoch [150/250], Loss: 96.9407, Validation Accuracy: 0.2677\n",
            "Epoch [175/250], Loss: 82.4920, Validation Accuracy: 0.2700\n",
            "Epoch [200/250], Loss: 73.3122, Validation Accuracy: 0.2677\n",
            "Epoch [225/250], Loss: 67.0864, Validation Accuracy: 0.2632\n",
            "Epoch [250/250], Loss: 61.5620, Validation Accuracy: 0.2654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings)\n",
        "    _, test_preds = torch.max(test_outputs, dim=1)\n",
        "    test_accuracy = accuracy_score(test_Y.cpu(), test_preds.cpu())  # Calculate accuracy\n",
        "print(f\"Test Accuracy: {test_accuracy:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuI-j6IIJnvK",
        "outputId": "1300de5d-eb86-4e9c-9a29-a859cecf3f9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.26544622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# After making predictions for validation or test set\n",
        "test_outputs2 = model(test_embeddings)\n",
        "_, test_preds2 = torch.max(test_outputs, dim=1)\n",
        "\n",
        "# Use sklearn to calculate classification report\n",
        "report = classification_report(test_Y.cpu(), test_preds2.cpu(), output_dict=True, zero_division=0)\n",
        "\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_Y.cpu(), test_preds2.cpu(), average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Test Macro-Precision: {precision:.4f}\")\n",
        "print(f\"Test Macro-Recall: {recall:.4f}\")\n",
        "print(f\"Test Macro-F1: {f1:.4f}\")\n",
        "# Extract macro-accuracy from the classification report\n",
        "macro_accuracy = report['accuracy']  # sklearn's accuracy report is macro-averaged for multi-class\n",
        "\n",
        "print(f\"Test Macro-Accuracy: {macro_accuracy:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3_MYSp0JGq8",
        "outputId": "47ff3214-7d49-4644-e45e-f3c5dbea4acf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Macro-Precision: 0.1605\n",
            "Test Macro-Recall: 0.1615\n",
            "Test Macro-F1: 0.1560\n",
            "Test Macro-Accuracy: 0.26544622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def mean_average_precision_at_k(predictions, actuals, k=25):\n",
        "    \"\"\"\n",
        "    Calculate the MAP@k for a set of predictions and actual labels.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of lists containing predicted labels, ranked from highest to lowest confidence.\n",
        "    - actuals: List of correct labels corresponding to the predictions.\n",
        "    - k: The cutoff rank for calculating MAP (default is 25).\n",
        "\n",
        "    Returns:\n",
        "    - The MAP@k score.\n",
        "    \"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for pred, actual in zip(predictions, actuals):\n",
        "        # We stop once we find the first correct label\n",
        "        precision_at_k = 0.0  # Initialize precision\n",
        "        for i in range(min(len(pred), k)):  # Loop up to k\n",
        "            if pred[i] == actual:\n",
        "                precision_at_k = 1 / (i + 1)  # Precision at rank i+1\n",
        "                break  # Stop once the correct label is found\n",
        "\n",
        "        average_precisions.append(precision_at_k)  # Append precision for this observation\n",
        "\n",
        "    # Calculate and return the mean average precision across all observations\n",
        "    return np.mean(average_precisions)\n",
        "\n",
        "\n",
        "# Modify your model evaluation code to return top 25 predictions\n",
        "def evaluate_model_map_at_25(model, test_embeddings, test_Y, k=25):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Get model outputs (logits) for test data\n",
        "        test_outputs = model(test_embeddings)\n",
        "\n",
        "        # Get the top 25 predictions for each test sample\n",
        "        topk_probs, topk_indices = torch.topk(test_outputs, k=k, dim=1)  # Get top-25 predictions\n",
        "\n",
        "        # Convert the predictions to numpy for processing\n",
        "        topk_preds = topk_indices.cpu().numpy()  # Shape: (num_samples, 25)\n",
        "        test_Y_np = test_Y.cpu().numpy()         # Ground truth labels\n",
        "\n",
        "        # Calculate MAP@25 using the mean_average_precision_at_k function\n",
        "        map_at_25 = mean_average_precision_at_k(topk_preds, test_Y_np, k=k)\n",
        "\n",
        "    print(f\"Test MAP@25: {map_at_25:.4f}\")\n",
        "    return map_at_25\n",
        "\n",
        "# Call this function during the test evaluation phase\n",
        "map_at_25 = evaluate_model_map_at_25(model, test_embeddings, test_Y, k=25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HahcNm-tJipk",
        "outputId": "257b690c-47a7-4315-9ce0-c9f177733dbc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAP@25: 0.4009\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}