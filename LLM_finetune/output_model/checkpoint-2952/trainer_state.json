{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2952,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1016260162601626,
      "grad_norm": 13.053667068481445,
      "learning_rate": 9.674796747967481e-05,
      "loss": 10.8379,
      "step": 100
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 10.072627067565918,
      "learning_rate": 9.339430894308943e-05,
      "loss": 6.028,
      "step": 200
    },
    {
      "epoch": 0.3048780487804878,
      "grad_norm": 6.910156726837158,
      "learning_rate": 9.000677506775068e-05,
      "loss": 5.6661,
      "step": 300
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 6.82485818862915,
      "learning_rate": 8.661924119241193e-05,
      "loss": 5.4407,
      "step": 400
    },
    {
      "epoch": 0.508130081300813,
      "grad_norm": 9.786774635314941,
      "learning_rate": 8.323170731707317e-05,
      "loss": 5.3972,
      "step": 500
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 8.72016716003418,
      "learning_rate": 7.984417344173442e-05,
      "loss": 5.3564,
      "step": 600
    },
    {
      "epoch": 0.7113821138211383,
      "grad_norm": 12.448028564453125,
      "learning_rate": 7.645663956639567e-05,
      "loss": 5.3395,
      "step": 700
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 9.692976951599121,
      "learning_rate": 7.306910569105691e-05,
      "loss": 5.2667,
      "step": 800
    },
    {
      "epoch": 0.9146341463414634,
      "grad_norm": 9.166146278381348,
      "learning_rate": 6.968157181571816e-05,
      "loss": 5.3059,
      "step": 900
    },
    {
      "epoch": 1.016260162601626,
      "grad_norm": 8.941445350646973,
      "learning_rate": 6.629403794037941e-05,
      "loss": 5.2785,
      "step": 1000
    },
    {
      "epoch": 1.1178861788617886,
      "grad_norm": 9.362371444702148,
      "learning_rate": 6.290650406504065e-05,
      "loss": 5.1538,
      "step": 1100
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 8.343155860900879,
      "learning_rate": 5.9518970189701894e-05,
      "loss": 5.2486,
      "step": 1200
    },
    {
      "epoch": 1.321138211382114,
      "grad_norm": 14.311344146728516,
      "learning_rate": 5.6131436314363154e-05,
      "loss": 5.1436,
      "step": 1300
    },
    {
      "epoch": 1.4227642276422765,
      "grad_norm": 8.529019355773926,
      "learning_rate": 5.2743902439024394e-05,
      "loss": 5.2081,
      "step": 1400
    },
    {
      "epoch": 1.524390243902439,
      "grad_norm": 12.409282684326172,
      "learning_rate": 4.935636856368564e-05,
      "loss": 5.1943,
      "step": 1500
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 12.72323226928711,
      "learning_rate": 4.596883468834689e-05,
      "loss": 5.1561,
      "step": 1600
    },
    {
      "epoch": 1.7276422764227641,
      "grad_norm": 10.076952934265137,
      "learning_rate": 4.2581300813008134e-05,
      "loss": 5.0774,
      "step": 1700
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 8.508747100830078,
      "learning_rate": 3.919376693766938e-05,
      "loss": 5.0779,
      "step": 1800
    },
    {
      "epoch": 1.9308943089430894,
      "grad_norm": 10.512468338012695,
      "learning_rate": 3.580623306233063e-05,
      "loss": 5.2058,
      "step": 1900
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 13.330016136169434,
      "learning_rate": 3.241869918699187e-05,
      "loss": 5.1051,
      "step": 2000
    },
    {
      "epoch": 2.1341463414634148,
      "grad_norm": 11.199150085449219,
      "learning_rate": 2.903116531165312e-05,
      "loss": 5.0657,
      "step": 2100
    },
    {
      "epoch": 2.2357723577235773,
      "grad_norm": 11.703444480895996,
      "learning_rate": 2.5643631436314363e-05,
      "loss": 5.1058,
      "step": 2200
    },
    {
      "epoch": 2.33739837398374,
      "grad_norm": 9.310635566711426,
      "learning_rate": 2.225609756097561e-05,
      "loss": 5.1027,
      "step": 2300
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 12.082961082458496,
      "learning_rate": 1.886856368563686e-05,
      "loss": 5.0596,
      "step": 2400
    },
    {
      "epoch": 2.540650406504065,
      "grad_norm": 12.059060096740723,
      "learning_rate": 1.5481029810298103e-05,
      "loss": 5.093,
      "step": 2500
    },
    {
      "epoch": 2.642276422764228,
      "grad_norm": 11.593711853027344,
      "learning_rate": 1.2093495934959351e-05,
      "loss": 4.963,
      "step": 2600
    },
    {
      "epoch": 2.7439024390243905,
      "grad_norm": 14.91281795501709,
      "learning_rate": 8.705962059620596e-06,
      "loss": 5.0271,
      "step": 2700
    },
    {
      "epoch": 2.845528455284553,
      "grad_norm": 11.201194763183594,
      "learning_rate": 5.318428184281843e-06,
      "loss": 5.0389,
      "step": 2800
    },
    {
      "epoch": 2.9471544715447155,
      "grad_norm": 10.82552433013916,
      "learning_rate": 1.9308943089430896e-06,
      "loss": 5.1026,
      "step": 2900
    }
  ],
  "logging_steps": 100,
  "max_steps": 2952,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0064272438591488e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
