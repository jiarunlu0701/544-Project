{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth = pd.read_json('groud_truth_embedding.json')\n",
    "targetStr = 'output/zero_shot_model_responses_qwen.csv'\n",
    "targetDf = pd.read_csv(targetStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 437/437 [00:40<00:00, 10.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()  # Set to evaluation mode\n",
    "def get_embedding(text):\n",
    "    # Tokenize and process the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Take the mean of the last hidden state to create a single embedding vector\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "embeddings = []\n",
    "for text in tqdm(targetDf['Generated Response'], desc=\"Generating embeddings\"):\n",
    "    embeddings.append(get_embedding(text))\n",
    "targetDf['Embeddings_Generated'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "misconception_df = pd.read_csv('../embedding_generator/misconception_mapping.csv')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_most_similar_id(target_embedding, ground_truth_df):\n",
    "    # Calculate cosine similarity between the target embedding and each embedding in ground_truth\n",
    "    similarities = cosine_similarity([target_embedding], list(ground_truth_df['Embedding']))\n",
    "    \n",
    "    # Find the index of the highest similarity score\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    # Retrieve the misconception_id with the highest similarity\n",
    "    most_similar_id = ground_truth_df.iloc[most_similar_index]['MisconceptionId']\n",
    "    return most_similar_id\n",
    "\n",
    "# Apply the function to each row in targetDf\n",
    "targetDf['prediction_result'] = targetDf['Embeddings_Generated'].apply(\n",
    "    lambda emb: find_most_similar_id(emb, ground_truth)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDf.rename(columns={'prediction_result': 'MisconceptionId'}, inplace=True)\n",
    "\n",
    "targetDf = targetDf.merge(misconception_df, on='MisconceptionId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002288329519450801\n"
     ]
    }
   ],
   "source": [
    "print(sum(targetDf['Expected Misconception'] == targetDf['MisconceptionName']) / len(targetDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Tom and Katie are incorrect.\n"
     ]
    }
   ],
   "source": [
    "print(targetDf['Generated Response'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Why is the given answer wrong under such circumstances?\n",
      "answer: Both Tom and Katie\n",
      "ConstructName: Factorise a quadratic expression in the form xÂ² - bx - c\n",
      "QuestionText: Tom and Katie are arguing about factorising. Tom says \\( x^{2}+5 x+6 \\equiv(x+3)(x+2) \\) \n",
      "Katie says \\( x^{2}-5 x-6 \\equiv(x-3)(x-2) \\) \n",
      "Who is correct?\n"
     ]
    }
   ],
   "source": [
    "print(targetDf['Prompt'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The given answer is incorrect because it does not accurately represent the function \\\\( y = x^2 + 4 \\\\). The provided image describes a function machine where the input \\\\( x \\\\) goes through a sequence of operations (addition and squaring), but it does not include the addition of 4 at any point, which is necessary for the equation \\\\( y = x^2 + 4 \\\\).'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetDf['Generated Response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction: Why is the given answer wrong under such circumstances?\\nanswer: ![A function machine which has 4 parts joined by arrows pointing from left to right. \"y\" is the first part, written on the left, followed by a horizontal arrow pointing to a rectangle that has \"+ 4\" written inside it, followed by a horizontal arrow pointing to a rectangle that has \"square\" written inside it, followed by a horizontal arrow pointing to \"ð‘¥\"]()\\nConstructName: Express a non-linear equation as a function machine\\nQuestionText: Which function machine matches the equation \\\\( y=x^{2}+4 ? \\\\)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetDf['Prompt'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
