{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth = pd.read_json('groud_truth_embedding.json')\n",
    "targetStr = 'output/zero_shot_model_responses_qwen.csv'\n",
    "targetDf = pd.read_csv(targetStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 437/437 [00:43<00:00,  9.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()  # Set to evaluation mode\n",
    "def get_embedding(text):\n",
    "    # Tokenize and process the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Take the mean of the last hidden state to create a single embedding vector\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "embeddings = []\n",
    "for text in tqdm(targetDf['Generated Response'], desc=\"Generating embeddings\"):\n",
    "    embeddings.append(get_embedding(text))\n",
    "targetDf['Embeddings_Generated'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "misconception_df = pd.read_csv('../embedding_generator/misconception_mapping.csv')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_most_similar_id(target_embedding, ground_truth_df):\n",
    "    # Calculate cosine similarity between the target embedding and each embedding in ground_truth\n",
    "    similarities = cosine_similarity([target_embedding], list(ground_truth_df['Embedding']))\n",
    "    \n",
    "    # Find the index of the highest similarity score\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    # Retrieve the misconception_id with the highest similarity\n",
    "    most_similar_id = ground_truth_df.iloc[most_similar_index]['MisconceptionId']\n",
    "    return most_similar_id\n",
    "\n",
    "# Apply the function to each row in targetDf\n",
    "targetDf['prediction_result'] = targetDf['Embeddings_Generated'].apply(\n",
    "    lambda emb: find_most_similar_id(emb, ground_truth)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDf.rename(columns={'prediction_result': 'MisconceptionId'}, inplace=True)\n",
    "\n",
    "targetDf = targetDf.merge(misconception_df, on='MisconceptionId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006864988558352402\n"
     ]
    }
   ],
   "source": [
    "print(sum(targetDf['Expected Misconception'] == targetDf['MisconceptionName']) / len(targetDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Tom and Katie are incorrect. The correct factorization for \\( x^2 + 5x + 6 \\) is \\( (x + 3)(x + 2) \\), and for \\( x^2 - 5x - 6 \\) it is \\( (x - 6)(x + 1) \\).\n"
     ]
    }
   ],
   "source": [
    "print(targetDf['Generated Response'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Why is the given answer wrong under such circumstances?\n",
      "answer: Both Tom and Katie\n",
      "ConstructName: Factorise a quadratic expression in the form xÂ² - bx - c\n",
      "QuestionText: Tom and Katie are arguing about factorising. Tom says \\( x^{2}+5 x+6 \\equiv(x+3)(x+2) \\) \n",
      "Katie says \\( x^{2}-5 x-6 \\equiv(x-3)(x-2) \\) \n",
      "Who is correct?\n"
     ]
    }
   ],
   "source": [
    "print(targetDf['Prompt'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided image does not accurately represent the function \\\\( y = x^2 + 4 \\\\). The image depicts a function machine with the operations of squaring (which would be represented by a square symbol or the exponent 2) and adding 4, but it lacks the initial variable \\\\( x \\\\) in the first step, and the final output should be \\\\( y \\\\) instead of \\\\( x \\\\).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetDf['Generated Response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction: Why is the given answer wrong under such circumstances?\\nanswer: ![A function machine which has 4 parts joined by arrows pointing from left to right. \"y\" is the first part, written on the left, followed by a horizontal arrow pointing to a rectangle that has \"+ 4\" written inside it, followed by a horizontal arrow pointing to a rectangle that has \"square\" written inside it, followed by a horizontal arrow pointing to \"ð‘¥\"]()\\nConstructName: Express a non-linear equation as a function machine\\nQuestionText: Which function machine matches the equation \\\\( y=x^{2}+4 ? \\\\)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetDf['Prompt'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
